### Network Security Project
ðŸš€ MLOps Pipeline Workflow
This project is built as an end-to-end MLOps pipeline that automates data ingestion, validation, transformation, model training, and deployment. Each stage is a distinct component that creates artifacts used by the next step

#### Project Structure

```
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ Network_Data
â”‚   â””â”€â”€ phisingData.csv
â”œâ”€â”€ README.md
â”œâ”€â”€ app.py
â”œâ”€â”€ data_schema
â”‚   â””â”€â”€ schema.yaml
â”œâ”€â”€ final_models
â”‚   â”œâ”€â”€ model.pkl
â”‚   â””â”€â”€ preprocessing.pkl
â”œâ”€â”€ main.py
â”œâ”€â”€ networksecurity
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cloud
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ components
â”‚   â”‚   â”œâ”€â”€ Data_ingestion.py
â”‚   â”‚   â”œâ”€â”€ Data_transformation.py
â”‚   â”‚   â”œâ”€â”€ Data_validation.py
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ model_trainer.py
â”‚   â”œâ”€â”€ constants
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ training_pipeline
â”‚   â”‚       â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ entity
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ artifact_entity.py
â”‚   â”‚   â””â”€â”€ config_entity.py
â”‚   â”œâ”€â”€ exception
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ exception.py
â”‚   â”œâ”€â”€ logging
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ logger.py
â”‚   â”œâ”€â”€ pipeline
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ batch_prediction.py
â”‚   â”‚   â””â”€â”€ training_pipeline.py
â”‚   â””â”€â”€ utils
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ main_utils
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â””â”€â”€ utils.py
â”‚       â””â”€â”€ ml_utils
â”‚           â”œâ”€â”€ __init__.py
â”‚           â”œâ”€â”€ metric
â”‚           â”‚   â”œâ”€â”€ __init__.py
â”‚           â”‚   â””â”€â”€ classification_metric.py
â”‚           â””â”€â”€ model
â”‚               â”œâ”€â”€ __init__.py
â”‚               â””â”€â”€ estimator.py
â”œâ”€â”€ notebooks
â”œâ”€â”€ prediction_output
â”‚   â””â”€â”€ output.csv
â”œâ”€â”€ push_data.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ templates
â”‚   â””â”€â”€ table.html
â”œâ”€â”€ test_mongodb.py
â””â”€â”€ valid_data
    â””â”€â”€ test.csv
```

## Steps followed during this project:

1. ðŸ“¥ Data Ingestion
- The pipeline begins by extracting data from the source.
- Source: Data is pulled from a MongoDB database.
- Process:
The Data Ingestion Component exports the specified collection to a "feature store" as a raw CSV file.
Based on a provided schema, unnecessary columns are dropped.
The cleaned data is then split into train.csv and test.csv files.

- Output: A Data Ingestion Artifact  is created, which contains the paths to the raw train.csv and test.csv files.
