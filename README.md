### Network Security Project
ğŸš€ MLOps Pipeline Workflow
This project is built as an end-to-end MLOps pipeline that automates data ingestion, validation, transformation, model training, and deployment. Each stage is a distinct component that creates artifacts used by the next step

### Project Flow

<img width="1363" height="427" alt="Image" src="https://github.com/user-attachments/assets/b1f43dbf-1c31-4435-bde0-7d2957ad935f" />

#### file Structure

```
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ Network_Data
â”‚   â””â”€â”€ phisingData.csv
â”œâ”€â”€ README.md
â”œâ”€â”€ app.py
â”œâ”€â”€ data_schema
â”‚   â””â”€â”€ schema.yaml
â”œâ”€â”€ final_models
â”‚   â”œâ”€â”€ model.pkl
â”‚   â””â”€â”€ preprocessing.pkl
â”œâ”€â”€ main.py
â”œâ”€â”€ networksecurity
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cloud
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ components
â”‚   â”‚   â”œâ”€â”€ Data_ingestion.py
â”‚   â”‚   â”œâ”€â”€ Data_transformation.py
â”‚   â”‚   â”œâ”€â”€ Data_validation.py
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ model_trainer.py
â”‚   â”œâ”€â”€ constants
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ training_pipeline
â”‚   â”‚       â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ entity
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ artifact_entity.py
â”‚   â”‚   â””â”€â”€ config_entity.py
â”‚   â”œâ”€â”€ exception
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ exception.py
â”‚   â”œâ”€â”€ logging
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ logger.py
â”‚   â”œâ”€â”€ pipeline
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ batch_prediction.py
â”‚   â”‚   â””â”€â”€ training_pipeline.py
â”‚   â””â”€â”€ utils
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ main_utils
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â””â”€â”€ utils.py
â”‚       â””â”€â”€ ml_utils
â”‚           â”œâ”€â”€ __init__.py
â”‚           â”œâ”€â”€ metric
â”‚           â”‚   â”œâ”€â”€ __init__.py
â”‚           â”‚   â””â”€â”€ classification_metric.py
â”‚           â””â”€â”€ model
â”‚               â”œâ”€â”€ __init__.py
â”‚               â””â”€â”€ estimator.py
â”œâ”€â”€ notebooks
â”œâ”€â”€ prediction_output
â”‚   â””â”€â”€ output.csv
â”œâ”€â”€ push_data.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ templates
â”‚   â””â”€â”€ table.html
â”œâ”€â”€ test_mongodb.py
â””â”€â”€ valid_data
    â””â”€â”€ test.csv
```

## Steps followed during this project:

1. ğŸ“¥ Data Ingestion
- The pipeline begins by extracting data from the source.
- Source: Data is pulled from a MongoDB database.
- Process:
The Data Ingestion Component exports the specified collection to a "feature store" as a raw CSV file.
Based on a provided schema, unnecessary columns are dropped.
The cleaned data is then split into train.csv and test.csv files.

- Output: A Data Ingestion Artifact  is created, which contains the paths to the raw train.csv and test.csv files.


<img width="1112" height="755" alt="Image" src="https://github.com/user-attachments/assets/255ea843-dbc0-45bf-8367-a25ce7e38716" />



2. âœ… Data Validation
- This step ensures the quality and integrity of the ingested data before any processing.
- Input: The Data Ingestion Artifact (the train.csv and test.csv files).
- Process:
Schema Validation: The Data Validation Component checks if the data matches the expected schema (e.g., correct number of columns , data types , and presence of all numerical columns ).
Data Drift Detection: It checks for significant changes in the data's distribution compared to a baseline. This is crucial for catching "model drift" before it happens.

- Output: A Data Validation Artifact is generated, which includes a status report and a drift report. If validation fails, the pipeline stops.


<img width="1331" height="835" alt="Image" src="https://github.com/user-attachments/assets/9786b402-522f-4913-9bd8-0e20f5bc250e" />


3. ğŸ› ï¸ Data Transformation
- Once validated, the raw data is preprocessed and made ready for model training.
- Input: The validated train.csv and test.csv files from the Data Validation Artifact.
- Process:
A data transformation Pipeline is created , which includes steps for Handling Missing Values.
KNN Imputer is used to fill in missing (NAN) values.
Robust Scaler  is applied to scale the features.
This pipeline is fit-transformed on the training data and transformed on the test data.

- Output: A Data Transformation Artifact is produced, containing the transformed data as numpy arrays (train.npy, test.npy) and the saved preprocessing.pkl object.


<img width="1039" height="858" alt="Image" src="https://github.com/user-attachments/assets/481553e8-e18a-4f1f-969d-7c45035c6904" />


4. ğŸ§  Model Trainer
- This component uses the preprocessed data to train and select the best-performing model.
- Input: The Data Transformation Artifact (specifically the train.npy and test.npy arrays).
- Process:
The numpy arrays are loaded and split into X_train, y_train, X_test, and y_test.
A Model Factory is used to train multiple models and find the best one.
The best model's score is compared against a pre-defined expected accuracy threshold.
If the model meets the threshold, it is saved as model.pkl.

- Output: A Model Trainer Artifact is created, which contains the final model.pkl and a metric artifact  with its performance details.


<img width="886" height="869" alt="Image" src="https://github.com/user-attachments/assets/5b28cd7d-d3ba-49c7-bf4a-5cc2520cef75" />


5. ğŸ“ˆ Model Evaluation
- The newly trained model is now compared against the model that is currently in production (if one exists).
- Input: The Model Trainer Artifact.
- Process: The Model Evaluation Component  assesses the new model. If its performance is better than the production model (or meets the acceptance criteria), it is marked as Model Accepted.

- Output: If "Yes", the pipeline proceeds to the final step. If "No", the pipeline stops, and the old model remains in production.


6. â¡ï¸ Model Pusher & Deployment
- Once a model is accepted, it is pushed to a central location for deployment.
- Process: The Model Pusher Component takes the accepted model artifacts and pushes them to a production location, such as a cloud bucket on AWS or Azure.
- CI/CD Deployment:
The entire "Network Security" application (including the API) is containerized into a Docker Image.
This image is pushed to AWS ECR (Elastic Container Registry).
- A GitHub Actions CI/CD pipeline automatically detects this new image, pulls it, and deploys it to a service like AWS EC2 or App Runner, making the new model available to users.
